{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRUM - Automated Model Serving Made Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We'll get our hands dirty by \n",
    "\n",
    "* Building a simple regression model using Scikit\n",
    "* Using DRUM for Batch Scoring\n",
    "* Using DRUM to get a REST API endpoint\n",
    "* Show a simple example app connected to the REST API\n",
    "* H2O, Keras, XGBoost, and DataRobot\n",
    "* Add a DataRobot remote agent if you are interested in further model monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "\n",
    "## load data\n",
    "\n",
    "df = pd.read_csv('../data/boston_housing.csv')\n",
    "df.head()\n",
    "\n",
    "## set features and target\n",
    "\n",
    "X = df.drop('MEDV', axis=1)\n",
    "y = df['MEDV']\n",
    "\n",
    "## train the model\n",
    "rf = RandomForestRegressor(n_estimators = 20)\n",
    "rf.fit(X,y)\n",
    "\n",
    "## serialize the model\n",
    "\n",
    "with open('../src/custom_model/rf.pkl', 'wb') as pkl:\n",
    "    pickle.dump(rf, pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4p0zDG-VWJP"
   },
   "source": [
    "# Batch Scoring with DRUM\n",
    "<a id=\"setup_complete\"></a>\n",
    "\n",
    "At this point our model has been written to disk and we want to start making predictions with it.  To do this, we'll leverage DRUM and it's ability to natively handle our scikit learn model, all we need to do is tell DRUM where it resides as well as the data we wish to score.  \n",
    "\n",
    "There are a lot of frameworks which DRUM supports nateively, but for those which DRUM doesn't support of these shelf, we'll just need to create some custom hooks so DRUM.  In this example, we'll highlight some very simple custom hooks, and will provide links to more complex examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C_OOeqEx6hqH",
    "outputId": "89c75e87-d13f-4f3d-c5e9-dec4c7e696e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\r\n"
     ]
    }
   ],
   "source": [
    "!drum score --code-dir ../src/custom_model --input ../data/boston_housing_inference.csv --output ../data/predictions.csv --target-type regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predictions\n",
       "0       24.475\n",
       "1       22.285\n",
       "2       34.935\n",
       "3       33.865\n",
       "4       35.685"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../data/predictions.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmS971iweH6t"
   },
   "source": [
    "# Start the inference server locally\n",
    "\n",
    "Batch scoring can be very useful, but the utility DRUM offers does not stop there.  We can also leverage DRUM to serve our model as a RESTful API endpoint.  The only thing that changes is the way we will structure the command - using the `server` mode instead of `score` model.  We'll also need to provide an address which is NOT in use.  \n",
    "\n",
    "When starting the server, we'll use `subprocess.Popen` so we may interact with the server in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "D7BrHC1gYjHD"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import time\n",
    "import os\n",
    "import datarobot as dr\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference_server = [\"drum\",\n",
    "              \"server\",\n",
    "              \"--code-dir\",\"../src/custom_model\", \n",
    "              \"--address\", \"0.0.0.0:6789\", \n",
    "              \"--show-perf\",\n",
    "              \"--target-type\", \"regression\",\n",
    "              \"--logging-level\", \"info\",\n",
    "              \"--show-stacktrace\",\n",
    "#               \"--verbose\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jWvksr_sYlEr"
   },
   "outputs": [],
   "source": [
    "inference_server = subprocess.Popen(run_inference_server, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peFenY-leJo3"
   },
   "source": [
    "## Ping the Server to make sure it is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "jmh7SRfQVnTU",
    "outputId": "0bc5161d-84dd-4c05-d0fe-0962f1c74c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check status\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'{\"message\":\"OK\"}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## confirm the server is running\n",
    "time.sleep(5) ## snoozing before pinging the server to give it time to actually start\n",
    "print('check status')\n",
    "requests.request(\"GET\", \"http://0.0.0.0:6789/\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOsaTgXOeNMG"
   },
   "source": [
    "## Send data to server for inference\n",
    "\n",
    "The request must provide our dataset as form data.  In order to do so, we'll create a simple python function to pass the data over appropriately.  We'll leverage the same function in our simple flask app a little later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iZ-sZcHMYmRx"
   },
   "outputs": [],
   "source": [
    "def score(data):\n",
    "    b_buf = BytesIO()\n",
    "    b_buf.write(data.to_csv(index=False).encode(\"utf-8\"))\n",
    "    b_buf.seek(0)\n",
    "  \n",
    "    url = \"http://localhost:6789/predict/\"\n",
    "    files = [\n",
    "        ('X', b_buf)\n",
    "    ]\n",
    "    response = requests.request(\"POST\", url, files = files, timeout=None, verify=False)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HjdKXUcUWUXq",
    "outputId": "5091625d-208d-4335-cfff-35d6449a6f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [24.475,\n",
      "                 22.285,\n",
      "                 34.935,\n",
      "                 33.865,\n",
      "                 35.685,\n",
      "                 27.225,\n",
      "                 21.735,\n",
      "                 24.675,\n",
      "                 15.845]}\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "scoring_data = pd.read_csv(\"../data/boston_housing_inference.csv\")\n",
    "predictions = score(scoring_data).json() ## score entire dataset but only show first 5 records\n",
    "pprint(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Flask App\n",
    "\n",
    "Now that we testing out our endpoing, now it is time to start up our flask app, but first, we need to set a few environment variables for the flask app.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n",
    "os.environ[\"LANG\"] = \"C.UTF-8\"\n",
    "os.environ[\"FLASK_APP\"] = \"server.app\"\n",
    "os.environ[\"FLASK_ENV\"] = \"development\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the flask app as follows will lock the interpreter and only return control once you interupt the kernal.  Be advised that interupting the kernel via the `stop` button will kill the flask app AND the inference server.  Once you execute the following cell, go over to the [app](http://localhost:8080/frontend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"server.app\" (lazy loading)\n",
      " * Environment: development\n",
      " * Debug mode: on\n",
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n",
      " * Restarting with stat\n",
      " * Debugger is active!\n",
      " * Debugger PIN: 208-528-333\n",
      "127.0.0.1 - - [26/Oct/2020 12:45:45] \"\u001b[37mGET /frontend HTTP/1.1\u001b[0m\" 200 -\n",
      "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO      B  LSTAT\n",
      "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.9   4.98\n",
      "\n",
      "[1 rows x 13 columns]\n",
      "making request\n",
      "prediciton [24.475]\n",
      "heylksdfmlsdmsdflklmsdfsdf\n",
      "127.0.0.1 - - [26/Oct/2020 12:45:51] \"\u001b[37mPOST /frontend HTTP/1.1\u001b[0m\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!cd ../src && python -m flask run --host 0.0.0.0 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server.terminate()\n",
    "# inference_server.stdout.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests.request(\"GET\", \"http://0.0.0.0:6789/\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Prop\n",
    "\n",
    "One may ask, what is the benefit to be had here?  Well, first of, there is not need for me to write an api to get the model up and running.  Second, DRUM allows me to abstract the framework away (provided I'm using one that is natively supported, or I can write enough python so that DRUM understands how to hook up to the model.  \n",
    "\n",
    "For example, I could hot swap models as I see fit (see exampels in `./src/other_models`)\n",
    "\n",
    "While we will run through several other frameworks with in `score` you can bet they are supported in `server` mode as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2O Mojo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "   Predictions\n",
      "0    24.504000\n",
      "1    22.492000\n",
      "2    34.554001\n",
      "3    34.420001\n",
      "4    35.289001\n",
      "5    28.394001\n",
      "6    21.936000\n",
      "7    23.451000\n",
      "8    17.065000\n"
     ]
    }
   ],
   "source": [
    "!drum score --code-dir ../src/other_models/h2o_mojo/regression --input ../data/boston_housing_inference.csv --target-type regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-26 12:46:32.054222: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-10-26 12:46:32.102805: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe33085c970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-10-26 12:46:32.102939: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-10-26 12:46:32,233 WARNING tensorflow:  No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Using TensorFlow backend.\n",
      "   Predictions\n",
      "0    23.668932\n",
      "1    23.421118\n",
      "2    31.283527\n",
      "3    33.996525\n",
      "4    33.757942\n",
      "5    28.036715\n",
      "6    20.675852\n",
      "7    19.578415\n",
      "8    19.676756\n"
     ]
    }
   ],
   "source": [
    "!drum score --code-dir ../src/other_models/python3_keras_joblib --input ../data/boston_housing_inference.csv --target-type regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "\n",
    "Requires XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "   Predictions\n",
      "0    24.541843\n",
      "1    21.260277\n",
      "2    34.018497\n",
      "3    32.569200\n",
      "4    34.248066\n",
      "5    27.282364\n",
      "6    20.803959\n",
      "7    19.645220\n",
      "8    16.968880\n"
     ]
    }
   ],
   "source": [
    "!drum score --code-dir ../src/other_models/python3_xgboost --input ../data/boston_housing_inference.csv --target-type regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataRobot Codegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Predictions\r\n",
      "0    24.258228\r\n",
      "1    24.258228\r\n",
      "2    32.451515\r\n",
      "3    32.451515\r\n",
      "4    32.451515\r\n",
      "5    24.258228\r\n",
      "6    21.078378\r\n",
      "7    13.107812\r\n",
      "8    13.107812\r\n"
     ]
    }
   ],
   "source": [
    "!drum score --code-dir ../src/other_models/dr_codegen --input ../data/boston_housing_inference.csv --target-type regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Deployments\n",
    "\n",
    "What follows will require a DataRobot account.  You can get a trial account at [https://www.datarobot.com/trial/](https://www.datarobot.com/trial/)\n",
    "\n",
    "Also, JDK 11 or 12 will be required.\n",
    "\n",
    "The main idea: we'll will start an agent service locally.  This agent will be monitoring a spooler.  The spooler could be something as simple as local file system, or a little more realistic like a message broker (pubsub, rabbitmq, sqs).  \n",
    "\n",
    "Once, this agent is spun up locally, we'll enable a few environment variables to let DRUM know that there is an agent present and that it needs to buffer data to defined spool.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the monitoring agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently - have to go in through the [UI](https://app2.datarobot.com/account/developer-tools) to grab the agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"your-token\"\n",
    "endpoint = \"https://app2.datarobot.com\"\n",
    "## connect to DataRobot platform with python client. \n",
    "client = dr.Client(token, \"{}/api/v2\".format(endpoint))\n",
    "# mlops_agents_tb = client.get(\"mlopsInstaller\")\n",
    "# with open(\"../mlops-agent.tar.gz\", \"wb\") as f:\n",
    "#     f.write(mlops_agents_tb.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Failed to set default locale\r\n"
     ]
    }
   ],
   "source": [
    "# !tar -xf ../mlops-agent.tar.gz -C ..\n",
    "!tar -xf ../datarobot-mlops-agent-6.2.4-399.tar.gz -C ..\n",
    "!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Agent\n",
    "\n",
    "When we'll configure the agent, we just need to define the DataRobot MLOPS location, our api token.  By default, the agent will expect the data to be spooled on the local file system.  Specifically, the default location will be `/tmp/ta` so we just need to make sure that location exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datarobot-mlops-agent-6.2.4-399.tar.gz'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_dir = next(filter(lambda x: \"datarobot-mlops-agent-6.2.4\" == x, os.listdir(\"..\")))\n",
    "with open(r'../{}/conf/mlops.agent.conf.yaml'.format(agents_dir)) as file:\n",
    "    documents = yaml.load(file, Loader=yaml.FullLoader)\n",
    "## configure the loaction of the mlops instance with which we'll communcate\n",
    "documents['mlopsUrl'] = endpoint\n",
    "# Set your API token\n",
    "documents['apiToken'] = token\n",
    "## write the configuration back to disk\n",
    "with open('../{}/conf/mlops.agent.conf.yaml'.format(agents_dir), \"w\") as f:\n",
    "    yaml.dump(documents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Agent Service\n",
    "\n",
    "Checking to make sure we can start up the agents service.  \n",
    "\n",
    "This will require a JDK - tested with 11 and 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run agents service\n",
    "subprocess.call(\"../{}/bin/start-agent.sh\".format(agents_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'DataRobot MLOps-Agent is running as a service.\\n']\n"
     ]
    }
   ],
   "source": [
    "## check status\n",
    "check = subprocess.Popen([\"../{}/bin/status-agent.sh\".format(agents_dir)], stdout=subprocess.PIPE)\n",
    "print(check.stdout.readlines())\n",
    "check.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2020-10-26 13:11:05,215 INFO  com.datarobot.mlops.agent.config.channels.YamlBuilder        [] - Found spooler of type FILESYSTEM\\n'\n",
      "b'2020-10-26 13:11:05,218 INFO  com.datarobot.mlops.agent.config.channels.YamlBuilder        [] - Setting directory = /tmp/ta\\n'\n",
      "b'2020-10-26 13:11:05,218 INFO  com.datarobot.mlops.agent.config.channels.YamlBuilder        [] - Setting CHANNEL_NAME = filesystem\\n'\n",
      "b'2020-10-26 13:11:05,904 ERROR com.datarobot.mlops.agent.Agent                              [] - Error during agent execution - Fail to get version from server  - {\"message\": \"Invalid Authorization header\"}\\n'\n",
      "b'2020-10-26 13:12:38,272 INFO  com.datarobot.mlops.agent.config.channels.YamlBuilder        [] - Found spooler of type FILESYSTEM\\n'\n",
      "b'2020-10-26 13:12:38,275 INFO  com.datarobot.mlops.agent.config.channels.YamlBuilder        [] - Setting directory = /tmp/ta\\n'\n",
      "b'2020-10-26 13:12:38,275 INFO  com.datarobot.mlops.agent.config.channels.YamlBuilder        [] - Setting CHANNEL_NAME = filesystem\\n'\n",
      "b\"2020-10-26 13:12:39,129 INFO  com.datarobot.mlops.common.client.MLOpsClient                [] - DataRobot Server API Version found: '2.22'\\n\"\n",
      "b\"2020-10-26 13:12:39,719 INFO  com.datarobot.mlops.common.client.MLOpsClient                [] - DataRobot Server API Version found: '2.22'\\n\"\n",
      "b\"2020-10-26 13:12:39,720 INFO  com.datarobot.mlops.agent.Agent                              [] - DataRobot server at 'https://app2.datarobot.com' is reachable.\\n\"\n",
      "b'2020-10-26 13:12:39,720 INFO  com.datarobot.mlops.agent.Agent                              [] - DataRobot Monitoring Agent will process 100 records at a time at most.\\n'\n",
      "b'2020-10-26 13:12:39,721 INFO  com.datarobot.mlops.agent.AgentRunner                        [] - Creating new agent channel\\n'\n",
      "b'2020-10-26 13:12:39,722 INFO  com.datarobot.mlops.agent.AgentRunner                        [] - Initializing agent channel\\n'\n",
      "b'2020-10-26 13:12:39,775 INFO  com.datarobot.mlops.agent.AgentRunner                        [] - Initialized\\n'\n",
      "b'2020-10-26 13:12:39,776 INFO  com.datarobot.mlops.agent.AgentRunner                        [] - Listing channels:\\n'\n",
      "b'\\tChannel Name: filesystem, Channel Type: FILESYSTEM\\n'\n",
      "b'\\n'\n",
      "b'2020-10-26 13:12:39,777 INFO  com.datarobot.mlops.agent.AgentRunner                        [] - Running agent as service process pid and hostname [87851@twhittaker-mb-G0ZUH]\\n'\n"
     ]
    }
   ],
   "source": [
    "## check log to see that the agent connected to DR MLOps\n",
    "check = subprocess.Popen([\"cat\", \"../{}/logs/mlops.agent.log\".format(agents_dir)], stdout=subprocess.PIPE)\n",
    "for line in check.stdout.readlines():\n",
    "    print(line)\n",
    "check.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataRobot MLOps - Deploying External Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To communication with DataRobot MLOps, with need to MLOps python client installed which came in the downloaded tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.4 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../datarobot-mlops-*/lib/datarobot_mlops-*.whl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarobot.mlops.mlops import MLOps\n",
    "from datarobot.mlops.common.enums import OutputType\n",
    "from datarobot.mlops.connected.client import MLOpsClient\n",
    "from datarobot.mlops.common.exception import DRConnectedException\n",
    "from datarobot.mlops.constants import Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYMENT_NAME=\"Boston Housing Prices ODSC\"\n",
    "TRAINING_DATA = '../data/boston_housing.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "        \"name\": \"Boston Housing Pricins\",\n",
    "        \"modelDescription\": {\n",
    "            \"description\": \"prediction price of home\"\n",
    "        },\n",
    "        \"target\": {\n",
    "            \"type\": \"Regression\",\n",
    "            \"name\": \"MEDV\",\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading training data - ../data/boston_housing.csv. This may take some time...\n",
      "Training dataset uploaded. Catalog ID 5f9703db10582b0047159b04.\n",
      "Create model package\n",
      "Deploy model package\n",
      "Enable feature drift\n",
      "\n",
      "Done.\n",
      "DEPLOYMENT_ID=5f9703fe4e13cd0177cb7eda, MODEL_ID=5f9703fc77a58303862423d7\n"
     ]
    }
   ],
   "source": [
    "# Create connected client\n",
    "mlops_client = MLOpsClient(endpoint, token)\n",
    "\n",
    "# Add training_data to model configuration\n",
    "print(\"Uploading training data - {}. This may take some time...\".format(TRAINING_DATA))\n",
    "dataset_id = mlops_client.upload_dataset(TRAINING_DATA)\n",
    "print(\"Training dataset uploaded. Catalog ID {}.\".format(dataset_id))\n",
    "model_info[\"datasets\"] = {\"trainingDataCatalogId\": dataset_id}\n",
    "\n",
    "# Create the model package\n",
    "print('Create model package')\n",
    "model_pkg_id = mlops_client.create_model_package(model_info)\n",
    "model_pkg = mlops_client.get_model_package(model_pkg_id)\n",
    "model_id = model_pkg[\"modelId\"]\n",
    "\n",
    "# Deploy the model package\n",
    "print('Deploy model package')\n",
    "deployment_id = mlops_client.deploy_model_package(model_pkg[\"id\"],\n",
    "                                                            DEPLOYMENT_NAME)\n",
    "\n",
    "# Enable data drift tracking\n",
    "print('Enable feature drift')\n",
    "enable_feature_drift = TRAINING_DATA is not None\n",
    "mlops_client.update_deployment_settings(deployment_id, target_drift=True,\n",
    "                                                  feature_drift=enable_feature_drift)\n",
    "_ = mlops_client.get_deployment_settings(deployment_id)\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"DEPLOYMENT_ID=%s, MODEL_ID=%s\" % (deployment_id, model_id))\n",
    "\n",
    "DEPLOYMENT_ID = deployment_id\n",
    "MODEL_ID = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_sIji-jleSr"
   },
   "source": [
    "# Adding Monitoring with MLOps Monitoring Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring With DRUM\n",
    "\n",
    "There are a few addition parameters we should set for the command line utility, or we may just create environment variables, and allow the drum utility to pick up the details from there.  \n",
    "\n",
    "```\n",
    "  --monitor             Monitor predictions using DataRobot MLOps. True or\n",
    "                        False. (env: MONITOR).Monitoring can not be used in\n",
    "                        unstructured mode.\n",
    "  --deployment-id DEPLOYMENT_ID\n",
    "                        Deployment id to use for monitoring model predictions\n",
    "                        (env: DEPLOYMENT_ID)\n",
    "  --model-id MODEL_ID   MLOps model id to use for monitoring predictions (env:\n",
    "                        MODEL_ID)\n",
    "  --monitor-settings MONITOR_SETTINGS\n",
    "                        MLOps setting to use for connecting with the MLOps\n",
    "                        Agent (env: MONITOR_SETTINGS)\n",
    "```\n",
    "For today, we'll set environment variables to add monitoring. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MONITOR\"] = \"True\"\n",
    "os.environ[\"DEPLOYMENT_ID\"] = deployment_id\n",
    "os.environ[\"MODEL_ID\"] = model_id\n",
    "os.environ[\"MONITOR_SETTINGS\"] = \"spooler_type=filesystem;directory=/tmp/ta;max_files=5;file_max_size=1045876000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://app2.datarobot.com/deployments/5f9703fe4e13cd0177cb7eda/overview'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{}/deployments/{}/overview\".format(endpoint, deployment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server_with_monitoring = subprocess.Popen(run_inference_server, stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../src && python -m flask run --host 0.0.0.0 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.call(\"../{}/bin/stop-agent.sh\".format(agents_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check that agent is stopped \n",
    "check = subprocess.Popen([\"../{}/bin/status-agent.sh\".format(agents_dir)], stdout=subprocess.PIPE)\n",
    "print(check.stdout.readlines())\n",
    "check.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = dr.Deployment.get(deployment_id)\n",
    "deployment\n",
    "deployment.get_service_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_stats = deployment.get_service_stats()\n",
    "service_stats.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Keras on GPU with DRUM and Monitoring Agent (DataRobot)",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}